# 프로젝트3 인프라 구축

# 1. VPC 구축

- VPC 설정
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/e74441a0-e494-46fe-99ad-73b1b82080ad/image.png)
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/7f4a7240-9cb8-4826-b0e0-b5d8636e16b0/image.png)
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/04b732a1-6cab-42c1-8b1b-cca79ee283a8/image.png)
    

# 2. 보안그룹 설정

- 보안그룹
    
    ## web-sg
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/dfe797a3-68fc-4965-9f2f-15d02774479d/image.png)
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/52a8f562-e042-4862-b346-09296c6c9865/image.png)
    

# 3. S3 버킷 생성

- 설정
    - 버킷이름 넣고
    - 모든퍼플릭액세스 차단
    - 버전관리: **비활성화**
    - 기본암호화: **Amazon S3 관리형 키(SSE-S3)를 사용한 서버 측 암호화**
    - 버킷 키: **활성화**
    - 객체 잠금: **비활성화**
    - 예시
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/afa64677-c6d3-4aed-86c7-20b8994389a7/image.png)
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/d7ad67f0-2ccb-4f78-9f3a-cfe91819b222/image.png)
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/ae8028f8-7a81-4433-8af9-e9b3bf9717f0/image.png)
        

# 4. API Gateway

- API 생성할 때는 REST API
    - 새 API 생성 선택
    - 엔드포인트 유형: 지역
- 메서드 생성, API배포
    
    ### **1. 메서드 생성**
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/69057e8c-822c-410c-b8c9-3f63911b2498/image.png)
    
    - 실행역할([api2kinesis](https://us-east-1.console.aws.amazon.com/iam/home?region=ap-northeast-2#/roles/details/api2kinesis)): apigateway관리, apigateway cloudwatch, kinesis, firehose 정책담긴 역할 넣어주기
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/0e791823-96f0-4745-b636-42e50ed33b7b/image.png)
    
    - **APIGateway 메서드 생성하고 난 후**에 통합요청 편집에서 URL 요청헤더 파라미터, 매핑 템플릿 수정
        - URL 요청 헤더 파라미터
            - 이름: Content-Type
            - 다음에서 매핑됨: 'application/x-amz-json-1.1’
        - 매핑 템플릿
            - 콘텐츠 유형: application/json
            - 템플릿 본문:
                
                ```python
                #set ( $enter = "
                ")
                #set($json = "$input.json('$')$enter")
                {
                    "Data": "$util.base64Encode("$json")",
                    "PartitionKey": "$input.params('X-Amzn-Trace-Id')",
                    "StreamName": "{kinesis-name}"  
                }
                ```
                
                ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/569c7868-4faa-4526-82a0-515f1d5a8e3d/image.png)
                
    
    ### **2. API 배포**
    
    - {} 중괄호형식으로 하면 해당 스테이지 이름 변수화 되어서 어느 스트림이든 연결가능
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/c0f371fb-9ffa-44b6-a440-8f24a61deb01/image.png)
        
    
- IAM 역할 설정 편집
    
    apigateway 실행역할(api2kinesis)의 신뢰관계를 아래와 같이 편집해야됨.
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/749db694-af42-4acf-b05d-744c04224215/image.png)
    

# 5. kinesis

- kinesis data stream 생성
    - 프로비저닝 샤드수 고정하고 생성
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/ef595ccf-150e-47ad-9379-463b32a81341/image.png)
    

# 6. firehose

- 생성
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/cd436f03-6f2d-419b-8373-a94ee2c40807/image.png)
    
    - **소스 설정에서 생성한 kinesis 선택**
    - **생성한 S3 선택, s3 버킷 접두사에 rawdata/ 입력**
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/b68ac560-aa4d-404b-bdf0-fc08fde4f413/image.png)
    
    - **firehose role에 s3 access role 추가**
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/038c9c83-159b-45b7-b369-2e78f7873a1a/image.png)
        
    

## 서버에서 테스트(API Gateway - Kinesis - Firehose - S3)

- curl 명령 테스트
    
    ```bash
    curl -d "{\"value\":\"30\",\"type\":\"Tip 3\"}" -H "Content-Type: application/json" -X POST https://js8ry2hzg5.execute-api.ap-northeast-2.amazonaws.com/PROD/dev/proj3-kinesis
    ```
    
- S3에 데이터 저장됐는지 확인
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/450ba119-bf2f-425e-8a6b-d0a18e264a6d/image.png)
    

# 7. glue db, crawler

- glue DB
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/3c0fa59d-d1b4-48ef-9f7a-b39233996854/image.png)
    
- clawler
    
    ## step1
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/6d2f8dfd-c46f-4109-9d73-f75fe0fc2637/image.png)
    
    ## step2: Add a data source
    
    - data source: S3
    - s3 path: 생성한 s3 링크
    - crawl new sub folders only 클릭
    - Add
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/faff06e6-c672-4c65-80d2-d634af2012a6/image.png)
    
    ## step3: 보안설정
    
    - IAM role 생성
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/96524542-9edf-4554-8b93-20c3b3d326ff/image.png)
    
    ## step4: DB 설정
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/351289a9-acc2-4bf8-8657-b2e51357031b/image.png)
    
- ETL job 스크립트
    
    ```python
    import sys
    from awsglue.transforms import *
    from awsglue.utils import getResolvedOptions
    from pyspark.context import SparkContext
    from awsglue.context import GlueContext
    from awsglue.job import Job
    from awsglue.dynamicframe import DynamicFrame
    from pyspark.sql.functions import col
    
    # Glue Context 초기화
    args = getResolvedOptions(sys.argv, ['JOB_NAME'])
    sc = SparkContext()
    glueContext = GlueContext(sc)
    spark = glueContext.spark_session
    job = Job(glueContext)
    job.init(args['JOB_NAME'], args)
    
    # Glue DB에서 테이블 로드
    datasource = glueContext.create_dynamic_frame.from_catalog(
        database="proj3-gluedb",  # Glue DB 이름
        table_name="1203_proj"    # Glue 테이블 이름
    )
    
    # DynamicFrame을 DataFrame으로 변환
    df = datasource.toDF()
    
    # 데이터 변환 (테이블 스키마에 맞게 처리)
    transformed_df = df \
        .withColumn("ip", col("ip")) \
        .withColumn("user_id", col("user_id")) \
        .withColumn("req_type", col("req_type")) \
        .withColumn("content_name", col("content_name")) \
        .withColumn("search_text", col("search_text")) \
        .withColumn("timestamp", col("timestamp")) \
    
    # 순서 정렬
    ordered_df = transformed_df.select(
        "user_id",
        "ip", 
        "req_type", 
        "content_name", 
        "search_text", 
        "timestamp"
    )
    
    # DataFrame을 DynamicFrame으로 변환
    dynamic_frame = DynamicFrame.fromDF(ordered_df, glueContext, "ordered_data")
    
    # S3로 데이터 저장
    glueContext.write_dynamic_frame.from_options(
        frame=dynamic_frame,
        connection_type="s3",
        connection_options={
            "path": "s3://glue-s3-1226/logs/",  # S3 저장 경로
            "partitionKeys": []  # 파티션 키가 필요하면 설정
        },
        format="csv"  # 저장할 포맷 (json, csv 등)
    )
    
    job.commit()
    
    ```
    

# 8. S3 to GlueDB(by Lambda)

- 역할 생성: **s3toGlue-lambda-role**
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/10156672-ab32-4e19-b45d-6568e36a6438/image.png)
    
- lambda 생성
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/af3e8d08-810c-4834-b68a-52bde8e04113/image.png)
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/78af3d95-2c21-4ca3-83d8-731810cc1629/image.png)
    
- 트리거 추가
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/dbd2ef0f-f54a-4639-9b5d-fee02d88e27b/image.png)
    
- 람다함수
    
    ```python
    import json
    import boto3
    import base64
    
    # AWS 클라이언트 설정
    s3_client = boto3.client('s3')
    glue_client = boto3.client('glue')
    
    # Glue 크롤러 이름 (사전에 생성해 두어야 함)
    CRAWLER_NAME = 's3Crawler'
    
    # Lambda 핸들러 함수
    def lambda_handler(event, context):
        # 이벤트에서 S3 버킷 이름과 객체 키를 추출합니다.
        bucket_name = event['Records'][0]['s3']['bucket']['name']
        object_key = event['Records'][0]['s3']['object']['key']
        
        print(f'New file {object_key} uploaded to bucket {bucket_name}')
    
        # Glue 크롤러 실행
        try:
            response = glue_client.start_crawler(Name=CRAWLER_NAME)
            print(f'Successfully started Glue crawler: s3Crawler')
        except glue_client.exceptions.CrawlerRunningException:
            print(f'Glue crawler s3Crawler is already running.')
        except Exception as e:
            print(f'Error starting Glue crawler: {str(e)}')
            raise
    
        return {
            'statusCode': 200,
            'body': json.dumps('Glue crawler started successfully')
        }
    ```
    

# 9. Glue Job 실행(glueDB → S3)

- glueETL JOB 스크립트
    
    ```python
    import sys
    from awsglue.transforms import *
    from awsglue.utils import getResolvedOptions
    from pyspark.context import SparkContext
    from awsglue.context import GlueContext
    from awsglue.job import Job
    from awsglue.dynamicframe import DynamicFrame
    from pyspark.sql.functions import col
    from pyspark.sql.functions import date_format
    
    # Glue Context 초기화
    args = getResolvedOptions(sys.argv, ['JOB_NAME'])
    sc = SparkContext()
    glueContext = GlueContext(sc)
    spark = glueContext.spark_session
    job = Job(glueContext)
    job.init(args['JOB_NAME'], args)
    
    # Glue DB에서 테이블 로드
    datasource = glueContext.create_dynamic_frame.from_catalog(
        database="proj3-gluedb",  # Glue DB 이름
        table_name="1203"    # Glue 테이블 이름
    )
    
    # DynamicFrame을 DataFrame으로 변환
    df = datasource.toDF()
    
    # 데이터 변환 (테이블 스키마에 맞게 처리)
    transformed_df = df \
        .withColumn("ip", col("ip")) \
        .withColumn("user_id", col("user_id")) \
        .withColumn("req_type", col("req_type")) \
        .withColumn("content_name", col("content_name")) \
        .withColumn("search_text", col("search_text")) \
        .withColumn("timestamp", date_format(col("timestamp"), "yyyyMMddHHmmss")) \
    
    # 순서 정렬
    ordered_df = transformed_df.select(
        "user_id",
        "ip", 
        "req_type", 
        "content_name", 
        "search_text", 
        "timestamp"
    )
    
    # DataFrame을 DynamicFrame으로 변환
    dynamic_frame = DynamicFrame.fromDF(ordered_df, glueContext, "ordered_data")
    
    # S3로 데이터 저장
    glueContext.write_dynamic_frame.from_options(
        frame=dynamic_frame,
        connection_type="s3",
        connection_options={
            "path": "s3://glue-s3-1226/logs/",  # S3 저장 경로
            "partitionKeys": []  # 파티션 키가 필요하면 설정
        },
        format="csv"  # 저장할 포맷 (json, csv 등)
    )
    
    job.commit()
    
    ```
    
- 스크립트 save 하고 **job run**

# 10. s3 to RDS

- RDS 서버 구축
    - 서브넷그룹
        - vpc: proj3-vpc
        - 가용영역: 2a,2c
    - 파라미터그룹
        - mySQL Community 8.0
    - 옵션그룹
        - mySQL 8.0
    - 데이터베이스 생성
        - 표준 생성
        - 엔진 유형: mySQL
        - 템플릿: 프리티어(추후 변경 가능)
        - DB 인스턴스 식별자: admin
        - 마스터 사용자 이름: admin
        - 마스터 암호: admin123!
        - 인스턴스: db.t3.micro
        - VPC: proj3-VPC
        - DB 서브넷 그룹: proj3-subnetgroup
        - 보안 그룹: db-sg
        - 가용영역: ap-northeast-2a
        - 추가구성
            - 초기 데이터베이스 이름: testdb
            - DB 파라미터 그룹: proj3-pg
            - 옵션그룹: proj3-og
            - 자동백업 : 비활성화
            - 암호화 x
            - 마이너 버전 자동 업그레이드 사용 : x
- EC2 서버에서 RDS 서버 테스트

```python
# db 설치
sudo yum install mariadb105

# mysql 접속
mysql -u admin -padmin123! -h DBendpoint
show databases;

```

- S3 to RDS Lambda 생성
    - glueDB에서 받은 S3 버킷으로 연결 후 생성
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/c5cea5b8-8bea-455b-bb20-16b7fbe168e9/image.png)
    

- lambda 함수 구성  설정
    - 환경변수
        
        
        | DB_NAME | testdb |
        | --- | --- |
        | DB_PASSWORD | admin123! |
        | DB_USERNAME | admin |
        | RDS_HOST | admin.cxkse86gm0ea.ap-northeast-2.rds.amazonaws.com |
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/17c3f8c8-720b-4e06-992e-1f6ac4eabcd0/image.png)
        
    - VPC 연결
        - 기존 DB 연결했던 가용영역의 서브넷과 연결
        - 보안그룹 : db-sg
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3752105d-7f6f-44d6-b5e3-09a051a33ea8/e53063c3-85bc-4a85-aecb-f40221048a0e/image.png)
        
    
- lambda 함수 코드
    
    ```python
    import json
    import boto3
    import pymysql
    import csv
    import os
    
    # RDS 설정 (환경 변수에서 가져오기)
    rds_host = os.environ['RDS_HOST']
    db_username = os.environ['DB_USERNAME']
    db_password = os.environ['DB_PASSWORD']
    db_name = os.environ['DB_NAME']
    db_table = os.environ['DB_TABLE']
    
    # S3 클라이언트 생성
    s3 = boto3.client('s3')
    
    # Lambda 핸들러 함수
    def lambda_handler(event, context):
        try:
            # 전달된 이벤트 로그 출력
            print("Received event:", json.dumps(event))
            
            # 'Records' 키 확인
            if 'Records' not in event:
                raise KeyError("'Records' 키가 이벤트 데이터에 없습니다.")
            
            # S3 이벤트 처리
            bucket = event['Records'][0]['s3']['bucket']['name']
            key = event['Records'][0]['s3']['object']['key']
            print(f"Bucket: {bucket}, Key: {key}")
    
        except KeyError as e:
            print(f"KeyError: {e}")
            return {
                'statusCode': 400,
                'body': json.dumps(f"Invalid event format: {str(e)}")
            }
        except Exception as e:
            print(f"Unhandled exception: {e}")
            raise e
            
        # S3에서 파일 다운로드
        download_path = f'/tmp/{key.split("/")[-1]}'
        s3.download_file(bucket, key, download_path)
        
        print(f"Downloading file from S3 bucket '{bucket}', key '{key}'")
    
        try:
            # MySQL 연결 설정
            connection = pymysql.connect(
                host=rds_host,
                user=db_username,
                password=db_password,
                database=db_name
            )
            print("MySQL 연결 성공")
        except Exception as e:
            print(f"MySQL 연결 실패: {e}")
            return {
                'statusCode': 500,
                'body': json.dumps(f"Database connection error: {str(e)}")
            }
        
        try:
            with connection.cursor() as cursor:
                # CSV 파일 읽기
                with open(download_path, 'r') as file:
                    csv_reader = csv.reader(file)
                    # 첫 번째 행에 헤더가 있다면 다음 줄로 넘어가기
                    next(csv_reader)
                    for row in csv_reader:
                        # 각 행을 RDS 테이블에 삽입
                        sql = f"""
                        INSERT INTO {db_table} (user_id, ip, req_type, content_name, search_text, timestamp) 
                        VALUES (%s, %s, %s, %s, %s, %s)
                        """
                        cursor.execute(sql, (int(row[0]), row[1], row[2], row[3], row[4], row[5]))
                connection.commit()
                print("데이터 삽입 완료")
        except Exception as e:
            print(f"RDS 데이터 삽입 중 오류 발생: {e}")
            return {
                'statusCode': 500,
                'body': json.dumps(f"Error inserting into RDS: {str(e)}")
            }
        finally:
            connection.close()
    
        return {
            'statusCode': 200,
            'body': json.dumps('Data stored successfully in RDS!')
        }
    
    ```
    
- 테이블 생성 쿼리
    
    ```sql
    CREATE TABLE user_logs (
        user_id INT NOT NULL,                 -- 사용자 ID (외래 키)
        ip VARCHAR(39) NOT NULL,              -- IPv4 또는 IPv6 주소를 저장
        req_type VARCHAR(20) NOT NULL,        -- 사용자 action
        content_name VARCHAR(100) NOT NULL,   -- 콘텐츠 이름
        search_text VARCHAR(100),             -- 검색 내용
        timestamp VARCHAR(30),
        CONSTRAINT fk_user_id FOREIGN KEY (user_id) REFERENCES user_info(user_id) ON DELETE CASCADE
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
    ```
